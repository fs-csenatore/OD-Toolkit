{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docs: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md\n",
    "\n",
    "Load Depenencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from object_detection.utils import label_map_util\n",
    "from tflite_support.metadata_writers import object_detector\n",
    "from tflite_support.metadata_writers import writer_utils\n",
    "from tflite_support import metadata\n",
    "import cv2\n",
    "from tflite_support import metadata as _metadata\n",
    "from tflite_support import metadata_schema_py_generated as _metadata_fb\n",
    "from tensorflow_lite_support.metadata.python.metadata_writers import metadata_info\n",
    "import json\n",
    "\n",
    "#Define Locations\n",
    "_PATH_TO_SAVED_MODEL_DIR = '/home/developer/Documents/git/Tensorflow/workspace/exported_model/tf2/BA/MED3_ssd_mobilenet_v2_fpn_640x640_postprocess/saved_model'\n",
    "_tflite_models_dir = pathlib.Path(_PATH_TO_SAVED_MODEL_DIR)\n",
    "\n",
    "_PATH_TO_DATASET = '/home/developer/Documents/git/Tensorflow/workspace/datasets/'\n",
    "_dataset_dir = pathlib.Path(_PATH_TO_DATASET)\n",
    "\n",
    "_ODT_LABEL_MAP_PATH = _dataset_dir/'dataset/MED3-REV1.00/MED3-REV1.00-detect_label_map.pptxt'\n",
    "_ODT_RECORD_FILE = _dataset_dir/'train.record'\n",
    "_TFLITE_LABEL_PATH = _tflite_models_dir/'tflite_label_map.txt'\n",
    "_TFLITE_MODEL_WITH_METADATA_PATH = _tflite_models_dir/'model_quant_metadata.tflite'\n",
    "_TFLITE_MODEL_PATH =  _tflite_models_dir/'model_quant.tflite'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define representative_dataset for weight and activation calibration\n",
    "\n",
    "Doc at: https://www.tensorflow.org/lite/performance/post_training_quantization\n",
    "\n",
    "The representative_dataset is a generator, that is used by TFLiteConverter.\n",
    "\n",
    "It uses the encoded images from a tfrecord file and provides a preprocessed input image for the calibration process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 10:36:11.783083: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/developer/Documents/git/Tensorflow/python-env/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-12-21 10:36:11.783168: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-12-21 10:36:11.783246: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (fs-development-machine): /proc/driver/nvidia/version does not exist\n",
      "2023-12-21 10:36:11.785076: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.TFRecordDataset(_ODT_RECORD_FILE)\n",
    "\n",
    "def decode_img(encoded_img):\n",
    "    image_np = tf.image.decode_jpeg(encoded_img, channels=3).numpy()\n",
    "    return image_np\n",
    "\n",
    "def preprocess_image(encoded_image ):\n",
    "    image = decode_img(encoded_image)\n",
    "    image = cv2.resize(image, (640,640))\n",
    "    image = (image.astype(np.float32) -127.5)/127.5\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    return image\n",
    "\n",
    "def representative_dataset():\n",
    "    \"\"\"\n",
    "    yield/return a input tensor for the model, we use the images from tfrecord\n",
    "    \"\"\"\n",
    "    for data in train_dataset.shuffle(1000):\n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(data.numpy())\n",
    "        encoded_image = example.features.feature['image/encoded'].bytes_list.value[0]\n",
    "        image_np = preprocess_image(encoded_image)\n",
    "        yield [image_np]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "Here are examples for different Post-Train-Quantization options.\n",
    "### Dynamic range quantization\n",
    "Dynamic range quantization is a recommended starting point because it provides reduced memory usage and faster computation without you having to provide a representative dataset for calibration. This type of quantization, statically quantizes only the weights from floating point to integer at conversion time, which provides 8-bits of precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 10:36:32.979271: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-12-21 10:36:32.979340: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-12-21 10:36:32.980842: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/developer/Documents/git/Tensorflow/workspace/exported_model/tf2/BA/MED3_ssd_resnet_v1_fpn_640x640_3k0/tflite/saved_model\n",
      "2023-12-21 10:36:33.097934: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-12-21 10:36:33.098007: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /home/developer/Documents/git/Tensorflow/workspace/exported_model/tf2/BA/MED3_ssd_resnet_v1_fpn_640x640_3k0/tflite/saved_model\n",
      "2023-12-21 10:36:33.367387: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2023-12-21 10:36:33.427423: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2023-12-21 10:36:34.763756: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /home/developer/Documents/git/Tensorflow/workspace/exported_model/tf2/BA/MED3_ssd_resnet_v1_fpn_640x640_3k0/tflite/saved_model\n",
      "2023-12-21 10:36:35.154022: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 2173182 microseconds.\n",
      "2023-12-21 10:36:36.297465: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-12-21 10:36:39.443156: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1989] Estimated count of arithmetic ops: 164.927 G  ops, equivalently 82.464 G  MACs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated count of arithmetic ops: 164.927 G  ops, equivalently 82.464 G  MACs\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(_PATH_TO_SAVED_MODEL_DIR)\n",
    "#converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further reduce latency during inference, \"dynamic-range\" operators dynamically quantize activations based on their range to 8-bits and perform computations with 8-bit weights and activations. This optimization provides latencies close to fully fixed-point inferences. However, the outputs are still stored using floating point so the increased speed of dynamic-range ops is less than a full fixed-point computation.\n",
    "\n",
    "### Full integer quantization\n",
    "\n",
    "You can get further latency improvements, reductions in peak memory usage, and compatibility with integer only hardware devices or accelerators by making sure all model math is integer quantized.\n",
    "\n",
    "For full integer quantization, you need to calibrate or estimate the range, i.e, (min, max) of all floating-point tensors in the model. Unlike constant tensors such as weights and biases, variable tensors such as model input, activations (outputs of intermediate layers) and model output cannot be calibrated unless we run a few inference cycles. As a result, the converter requires a representative dataset to calibrate them. This dataset can be a small subset (around ~100-500 samples) of the training or validation data. Refer to the representative_dataset() function above.\n",
    "\n",
    "#### Integer with float fallback (using default float input/output)\n",
    "In order to fully integer quantize a model, but use float operators when they don't have an integer implementation (to ensure conversion occurs smoothly), use the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(_PATH_TO_SAVED_MODEL_DIR)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This tflite_quant_model won't be compatible with integer only devices (such as 8-bit microcontrollers) and accelerators (such as the Coral Edge TPU) because the input and output still remain float in order to have the same interface as the original float only model.\n",
    "\n",
    "#### Integer only\n",
    "\n",
    "Creating integer only models is a common use case for TensorFlow Lite for Microcontrollers and Coral Edge TPUs.\n",
    "Additionally, to ensure compatibility with integer only devices (such as 8-bit microcontrollers) and accelerators (such as the Coral Edge TPU), you can enforce full integer quantization for all ops including the input and output, by using the following steps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(_PATH_TO_SAVED_MODEL_DIR)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "#converter.inference_input_type = tf.uint8  # or tf.int8\n",
    "#converter.inference_output_type = tf.uint8  # or tf.int8\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integer only: 16-bit activations with 8-bit weights (experimental)\n",
    "\n",
    "This is an experimental quantization scheme. It is similar to the \"integer only\" scheme, but activations are quantized based on their range to 16-bits, weights are quantized in 8-bit integer and bias is quantized into 64-bit integer. This is referred to as 16x8 quantization further.\n",
    "\n",
    "The main advantage of this quantization is that it can improve accuracy significantly, but only slightly increase model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(_PATH_TO_SAVED_MODEL_DIR)\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n",
    "                                       tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The disadvantage of this quantization is:\n",
    "\n",
    "    Currently inference is noticeably slower than 8-bit full integer due to the lack of optimized kernel implementation.\n",
    "    Currently it is incompatible with the existing hardware accelerated TFLite delegates, but it seems to work with ethos-u\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show Input and Output types of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input 0 = <class 'numpy.float32'>\n",
      "output 0 = <class 'numpy.float32'>\n",
      "output 1 = <class 'numpy.float32'>\n",
      "output 2 = <class 'numpy.float32'>\n",
      "output 3 = <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_quant_model)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input 0 =', input_type)\n",
    "for i in range (0,4):\n",
    "    output_type = interpreter.get_output_details()[i]['dtype']\n",
    "    print('output',i , '=', output_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204400632"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model_quant_file = _TFLITE_MODEL_PATH\n",
    "tflite_model_quant_file.write_bytes(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Include META_DATA into tflite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Labe Map\n",
    "category_index = label_map_util.create_category_index_from_labelmap(_ODT_LABEL_MAP_PATH)\n",
    "f = open(_TFLITE_LABEL_PATH, 'w')\n",
    "\n",
    "#Range are all  label ids in label map\n",
    "for class_id in range(1, 32+1):\n",
    "  if class_id not in category_index:\n",
    "    f.write('???\\n')\n",
    "    continue\n",
    "  \n",
    "  name = category_index[class_id]['name']\n",
    "  f.write(name+'\\n')\n",
    "  \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_MODEL_NAME = \"ObjectDetector\"\n",
    "_MODEL_DESCRIPTION = (\n",
    "    \"Identify which of a known set of objects might be present and provide \"\n",
    "    \"information about their positions within the given image or a video \"\n",
    "    \"stream.\")\n",
    "_INPUT_NAME = \"image\"\n",
    "_INPUT_DESCRIPTION = \"Input image to be detected.\"\n",
    "\n",
    "_OUTPUT_CATRGORY_NAME = \"category\"\n",
    "_OUTPUT_CATEGORY_DESCRIPTION = \"The categories of the detected boxes.\"\n",
    "_OUTPUT_SCORE_NAME = \"score\"\n",
    "_OUTPUT_SCORE_DESCRIPTION = \"The scores of the detected boxes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create general info\n",
    "general_md = metadata_info.GeneralMd(\n",
    "    name=\"ResNet-V1 \" + _MODEL_NAME,\n",
    "    version=\"V1\",\n",
    "    description=_MODEL_DESCRIPTION,\n",
    "    author=\"F&S Elektronik Systeme GmbH\"\n",
    ")\n",
    "\n",
    "# Creates input info\n",
    "input_md = metadata_info.InputImageTensorMd(\n",
    "    name=_INPUT_NAME,\n",
    "    description=\"VGG\",\n",
    "    norm_mean=(127.5,),\n",
    "    norm_std=(127.5,),\n",
    "    color_space_type=_metadata_fb.ColorSpaceType.RGB,\n",
    "    tensor_type=writer_utils.get_input_tensor_types(writer_utils.load_file(_TFLITE_MODEL_PATH))[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MobileNet V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create general info\n",
    "general_md = metadata_info.GeneralMd(\n",
    "    name=\"MobileNet-V1 \"+ _MODEL_NAME,\n",
    "    version=\"V1\",\n",
    "    description=_MODEL_DESCRIPTION,\n",
    "    author=\"F&S Elektronik Systeme GmbH\"\n",
    ")\n",
    "\n",
    "# Creates input info\n",
    "input_md = metadata_info.InputImageTensorMd(\n",
    "    name=_INPUT_NAME,\n",
    "    description=\"normalized\",\n",
    "    norm_mean=(0,),\n",
    "    norm_std=(1,),\n",
    "    color_space_type=_metadata_fb.ColorSpaceType.RGB ,\n",
    "    tensor_type=writer_utils.get_input_tensor_types(writer_utils.load_file(_TFLITE_MODEL_PATH))[0])\n",
    "\n",
    "input_md.color_space_type = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates output info.\n",
    "output_category_md = metadata_info.CategoryTensorMd(\n",
    "    name=_OUTPUT_CATRGORY_NAME,\n",
    "    description=_OUTPUT_CATEGORY_DESCRIPTION,\n",
    "    label_files=[\n",
    "        metadata_info.LabelFileMd(file_path=file_path)\n",
    "        for file_path in [_TFLITE_LABEL_PATH]\n",
    "    ])\n",
    "\n",
    "output_score_md = metadata_info.ClassificationTensorMd(\n",
    "    name=_OUTPUT_SCORE_NAME,\n",
    "    description=_OUTPUT_SCORE_DESCRIPTION,\n",
    "    score_calibration_md=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Include Metadata\n",
    "writer = object_detector.MetadataWriter.create_from_metadata_info(\n",
    "    writer_utils.load_file(_TFLITE_MODEL_PATH),\n",
    "    general_md=general_md, input_md=input_md,\n",
    "    output_category_md=output_category_md,\n",
    "    output_score_md=output_score_md\n",
    ")\n",
    "writer_utils.save_file(writer.populate(), _TFLITE_MODEL_WITH_METADATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata populated:\n",
      "{\n",
      "  \"name\": \"ResNet-V1 ObjectDetector\",\n",
      "  \"description\": \"Identify which of a known set of objects might be present and provide information about their positions within the given image or a video stream.\",\n",
      "  \"version\": \"V1\",\n",
      "  \"subgraph_metadata\": [\n",
      "    {\n",
      "      \"input_tensor_metadata\": [\n",
      "        {\n",
      "          \"name\": \"image\",\n",
      "          \"description\": \"VGG\",\n",
      "          \"content\": {\n",
      "            \"content_properties_type\": \"ImageProperties\",\n",
      "            \"content_properties\": {\n",
      "              \"color_space\": \"RGB\"\n",
      "            }\n",
      "          },\n",
      "          \"process_units\": [\n",
      "            {\n",
      "              \"options_type\": \"NormalizationOptions\",\n",
      "              \"options\": {\n",
      "                \"mean\": [\n",
      "                  127.5\n",
      "                ],\n",
      "                \"std\": [\n",
      "                  127.5\n",
      "                ]\n",
      "              }\n",
      "            }\n",
      "          ],\n",
      "          \"stats\": {\n",
      "            \"max\": [\n",
      "              1.0\n",
      "            ],\n",
      "            \"min\": [\n",
      "              -1.0\n",
      "            ]\n",
      "          }\n",
      "        }\n",
      "      ],\n",
      "      \"output_tensor_metadata\": [\n",
      "        {\n",
      "          \"name\": \"score\",\n",
      "          \"description\": \"The scores of the detected boxes.\",\n",
      "          \"content\": {\n",
      "            \"content_properties_type\": \"FeatureProperties\",\n",
      "            \"content_properties\": {\n",
      "            },\n",
      "            \"range\": {\n",
      "              \"min\": 2,\n",
      "              \"max\": 2\n",
      "            }\n",
      "          },\n",
      "          \"stats\": {\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"location\",\n",
      "          \"description\": \"The locations of the detected boxes.\",\n",
      "          \"content\": {\n",
      "            \"content_properties_type\": \"BoundingBoxProperties\",\n",
      "            \"content_properties\": {\n",
      "              \"index\": [\n",
      "                1,\n",
      "                0,\n",
      "                3,\n",
      "                2\n",
      "              ],\n",
      "              \"type\": \"BOUNDARIES\"\n",
      "            },\n",
      "            \"range\": {\n",
      "              \"min\": 2,\n",
      "              \"max\": 2\n",
      "            }\n",
      "          },\n",
      "          \"stats\": {\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"number of detections\",\n",
      "          \"description\": \"The number of the detected boxes.\",\n",
      "          \"content\": {\n",
      "            \"content_properties_type\": \"FeatureProperties\",\n",
      "            \"content_properties\": {\n",
      "            }\n",
      "          },\n",
      "          \"stats\": {\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"category\",\n",
      "          \"description\": \"The categories of the detected boxes.\",\n",
      "          \"content\": {\n",
      "            \"content_properties_type\": \"FeatureProperties\",\n",
      "            \"content_properties\": {\n",
      "            },\n",
      "            \"range\": {\n",
      "              \"min\": 2,\n",
      "              \"max\": 2\n",
      "            }\n",
      "          },\n",
      "          \"stats\": {\n",
      "          },\n",
      "          \"associated_files\": [\n",
      "            {\n",
      "              \"name\": \"tflite_label_map.txt\",\n",
      "              \"description\": \"Labels for categories that the model can recognize.\",\n",
      "              \"type\": \"TENSOR_VALUE_LABELS\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"output_tensor_groups\": [\n",
      "        {\n",
      "          \"name\": \"detection_result\",\n",
      "          \"tensor_names\": [\n",
      "            \"location\",\n",
      "            \"category\",\n",
      "            \"score\"\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"author\": \"F&S Elektronik Systeme GmbH\",\n",
      "  \"min_parser_version\": \"1.2.0\"\n",
      "}\n",
      "\n",
      "=============================\n",
      "Associated file(s) populated:\n",
      "['tflite_label_map.txt']\n",
      "VGG\n"
     ]
    }
   ],
   "source": [
    "displayer = metadata.MetadataDisplayer.with_model_file(_TFLITE_MODEL_WITH_METADATA_PATH)\n",
    "print(\"Metadata populated:\")\n",
    "print(displayer.get_metadata_json())\n",
    "print(\"=============================\")\n",
    "print(\"Associated file(s) populated:\")\n",
    "print(displayer.get_packed_associated_file_list())\n",
    "data = json.loads(displayer.get_metadata_json())\n",
    "print(data[\"subgraph_metadata\"][0][\"input_tensor_metadata\"][0][\"description\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(str(_TFLITE_MODEL_WITH_METADATA_PATH))\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)\n",
    "\n",
    "print('input_details = \\n', interpreter.get_input_details())\n",
    "print('output_details = \\n', interpreter.get_output_details())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
